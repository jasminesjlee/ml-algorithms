{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14d8b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, num_iters, lr):\n",
    "        self.w = None # (p+1, 1)\n",
    "        self.num_iters = num_iters\n",
    "        self.lr = lr\n",
    "        \n",
    "    def loss(self, Y, Y_pred):\n",
    "        return (np.sum(np.square(Y_pred - Y))) / Y.shape[0]\n",
    "    \n",
    "    def loss_gradient(self, Y, X):\n",
    "        # loss = (1/n) sum (xi^Tw - yi)**2 \n",
    "        # gradient of loss: (2/n) sum (xi^Tw - yi) * xi\n",
    "        Y_pred = np.dot(X, self.w) # (n, 1)\n",
    "        gradients = np.zeros((self.w.shape[0], 1))\n",
    "        for y, y_pred, x in zip(Y, Y_pred, X):\n",
    "            # y_pred : (1, 1). y : (1,1). x: (1, p)\n",
    "            curr_grad = (2/Y.shape[0]) * np.dot((y_pred[0] - y[0]), x.T.reshape(self.w.shape))\n",
    "            gradients += curr_grad\n",
    "        return (1/Y.shape[0]) * gradients\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        # X: (n, p) matrix\n",
    "        # Y: (n, 1) matrix\n",
    "        n, p = X.shape\n",
    "        \n",
    "        # adds bias term to X by adding feat with value 1. X goes from (n,p) to (n,p+1)\n",
    "        X = np.hstack((X, np.ones((n, 1))))\n",
    "        \n",
    "        # adds bias term to w by adding a feature. w goes from (p,1) to (p+1,1)\n",
    "        self.w = np.zeros((p+1, 1))\n",
    "        \n",
    "        for _ in range(self.num_iters):            \n",
    "            # here, depends on if you want to do SGD or GD or batch GD.\n",
    "            # we will do GD\n",
    "            Y_pred = np.dot(X, self.w) # (n, 1) matrix\n",
    "            self.w -= self.lr * self.loss_gradient(Y, X)\n",
    "            print(f\"Current iteration loss: {self.loss(Y, Y_pred)}\")\n",
    "    \n",
    "    def predict(self, X, Y):\n",
    "        n, p = X.shape\n",
    "        X = np.hstack((X, np.ones((n, 1))))\n",
    "        Y_pred = np.dot(X, self.w) # (n, 1) matrix\n",
    "        print(f\"Loss = {self.loss(Y, Y_pred)}\")\n",
    "        print(f\"True labels: {Y}\")\n",
    "        print(f\"Pred labels: {Y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6ab5e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration loss: 15.166666666666666\n",
      "Current iteration loss: 0.07446502057613165\n",
      "Current iteration loss: 0.009270350048265005\n",
      "Current iteration loss: 0.008773262945776973\n",
      "Current iteration loss: 0.008559936433970235\n",
      "Current iteration loss: 0.008352938358936397\n",
      "Current iteration loss: 0.008150950866275182\n",
      "Current iteration loss: 0.007953847777418458\n",
      "Current iteration loss: 0.007761510958045657\n",
      "Current iteration loss: 0.007573825152009422\n",
      "Loss = 0.007390677890333659\n",
      "True labels: [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [6]]\n",
      "Pred labels: [[1.15020201]\n",
      " [2.10442544]\n",
      " [3.05864887]\n",
      " [4.0128723 ]\n",
      " [4.96709573]\n",
      " [5.92131916]]\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(10, 0.2)\n",
    "X = np.array([[1], [2], [3], [4], [5], [6]])\n",
    "y = np.array([[1], [2], [3], [4], [5], [6]])\n",
    "lr.fit(X, y)\n",
    "lr.predict(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "2aa8a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    # https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
    "    def __init__(self, num_iters, lr):\n",
    "        self.w = None # (p+1, 1)\n",
    "        self.num_iters = num_iters\n",
    "        self.lr = lr\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def loss(self, Y, Y_pred):\n",
    "        # binary cross entropy loss\n",
    "        return -1/Y.shape[0] * (np.sum(Y * np.log(Y_pred) + (1-Y) * np.log(1-Y_pred)))\n",
    "    \n",
    "    def loss_gradient(self, Y, Y_pred, X):\n",
    "        # Y-pred: (n,1)\n",
    "        # Y: (n,1)\n",
    "        # X: (n,p)\n",
    "        # output: (p, 1)\n",
    "        #transform to: (n, 1)\n",
    "        gradient_per_feat = (2/Y.shape[0]) * np.dot(X.T, (Y_pred - Y))\n",
    "        return gradient_per_feat.reshape(self.w.shape)\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        # X: (n, p)\n",
    "        # Y: (n, 1)\n",
    "        # w: (p, 1)\n",
    "        n, p = X.shape\n",
    "        X = np.hstack((X, np.ones((n, 1))))\n",
    "        self.w = np.zeros((p+1, 1))\n",
    "        \n",
    "        for _ in range(self.num_iters):\n",
    "            Y_pred = self.sigmoid(np.dot(X, self.w))\n",
    "            print(f\"curr iteration loss: {self.loss(Y, Y_pred)}\")\n",
    "            lg = self.loss_gradient(Y, Y_pred, X)\n",
    "            self.w -= self.lr * lg\n",
    "            \n",
    "    def decision_boundary(self, prob):\n",
    "        return 1 if prob >= .5 else 0\n",
    "    \n",
    "    def predict(self, X, Y):\n",
    "        n, p = X.shape\n",
    "        X = np.hstack((X, np.ones((n, 1))))\n",
    "        Y_pred = self.sigmoid(np.dot(X, self.w))\n",
    "        \n",
    "        decision_boundary = np.vectorize(self.decision_boundary)\n",
    "        print(Y)\n",
    "        print(decision_boundary(Y_pred))\n",
    "        print(f\"curr iteration loss: {self.loss(Y, Y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "9645b6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr iteration loss: 0.6931471805599452\n",
      "curr iteration loss: 0.680523187218805\n",
      "curr iteration loss: 0.6030037231000347\n",
      "curr iteration loss: 0.5861778505153081\n",
      "curr iteration loss: 0.5635648277944661\n",
      "curr iteration loss: 0.5480646829293379\n",
      "curr iteration loss: 0.5318260845147702\n",
      "curr iteration loss: 0.5181209899550075\n",
      "curr iteration loss: 0.5048929195256662\n",
      "curr iteration loss: 0.4928291128584051\n",
      "curr iteration loss: 0.4813995056792133\n",
      "curr iteration loss: 0.4706687683890909\n",
      "curr iteration loss: 0.46049073457446604\n",
      "curr iteration loss: 0.450830078503481\n",
      "curr iteration loss: 0.4416325264008363\n",
      "curr iteration loss: 0.4328639714630258\n",
      "curr iteration loss: 0.4244939403228932\n",
      "curr iteration loss: 0.4164967437535292\n",
      "curr iteration loss: 0.40884963853248407\n",
      "curr iteration loss: 0.4015318190099877\n",
      "curr iteration loss: 0.39452418209298423\n",
      "curr iteration loss: 0.38780891681628404\n",
      "curr iteration loss: 0.38136943474424645\n",
      "curr iteration loss: 0.37519022175097716\n",
      "curr iteration loss: 0.36925677905849275\n",
      "curr iteration loss: 0.36355554360506487\n",
      "curr iteration loss: 0.35807382951801214\n",
      "curr iteration loss: 0.35279976713802264\n",
      "curr iteration loss: 0.34772224859573764\n",
      "curr iteration loss: 0.3428308755156725\n",
      "curr iteration loss: 0.33811591046799283\n",
      "curr iteration loss: 0.3335682313545717\n",
      "curr iteration loss: 0.32917928888380943\n",
      "curr iteration loss: 0.32494106689834146\n",
      "curr iteration loss: 0.32084604544888573\n",
      "curr iteration loss: 0.31688716645201725\n",
      "curr iteration loss: 0.313057801784244\n",
      "curr iteration loss: 0.3093517236601502\n",
      "curr iteration loss: 0.3057630771470269\n",
      "curr iteration loss: 0.30228635467260406\n",
      "curr iteration loss: 0.2989163723883895\n",
      "curr iteration loss: 0.2956482482576257\n",
      "curr iteration loss: 0.2924773817438473\n",
      "curr iteration loss: 0.28939943498320647\n",
      "curr iteration loss: 0.2864103153309553\n",
      "curr iteration loss: 0.28350615917959826\n",
      "curr iteration loss: 0.2806833169531685\n",
      "curr iteration loss: 0.2779383391887621\n",
      "curr iteration loss: 0.2752679636228409\n",
      "curr iteration loss: 0.2726691032058663\n",
      "curr iteration loss: 0.27013883497452695\n",
      "curr iteration loss: 0.26767438971617835\n",
      "curr iteration loss: 0.26527314236511346\n",
      "curr iteration loss: 0.26293260307494803\n",
      "curr iteration loss: 0.26065040891573976\n",
      "curr iteration loss: 0.25842431614847616\n",
      "curr iteration loss: 0.2562521930332884\n",
      "curr iteration loss: 0.2541320131311836\n",
      "curr iteration loss: 0.25206184906226375\n",
      "curr iteration loss: 0.2500398666863184\n",
      "curr iteration loss: 0.24806431967437753\n",
      "curr iteration loss: 0.24613354444228747\n",
      "curr iteration loss: 0.2442459554196509\n",
      "curr iteration loss: 0.2424000406295746\n",
      "curr iteration loss: 0.2405943575565888\n",
      "curr iteration loss: 0.2388275292818776\n",
      "curr iteration loss: 0.23709824086658515\n",
      "curr iteration loss: 0.2354052359654556\n",
      "curr iteration loss: 0.2337473136544405\n",
      "curr iteration loss: 0.23212332545716458\n",
      "curr iteration loss: 0.2305321725563047\n",
      "curr iteration loss: 0.22897280317699634\n",
      "curr iteration loss: 0.22744421013036492\n",
      "curr iteration loss: 0.2259454285061756\n",
      "curr iteration loss: 0.2244755335044251\n",
      "curr iteration loss: 0.22303363839645765\n",
      "curr iteration loss: 0.22161889260688833\n",
      "curr iteration loss: 0.2202304799082615\n",
      "curr iteration loss: 0.2188676167209636\n",
      "curr iteration loss: 0.217529550511458\n",
      "curr iteration loss: 0.21621555828241096\n",
      "curr iteration loss: 0.2149249451487424\n",
      "curr iteration loss: 0.2136570429940633\n",
      "curr iteration loss: 0.2124112092023547\n",
      "curr iteration loss: 0.21118682546011008\n",
      "curr iteration loss: 0.20998329662449491\n",
      "curr iteration loss: 0.20880004965339305\n",
      "curr iteration loss: 0.20763653259349016\n",
      "curr iteration loss: 0.20649221362281756\n",
      "curr iteration loss: 0.20536658014441606\n",
      "curr iteration loss: 0.2042591379280158\n",
      "curr iteration loss: 0.20316941029682983\n",
      "curr iteration loss: 0.2020969373567606\n",
      "curr iteration loss: 0.2010412752654947\n",
      "curr iteration loss: 0.20000199553913\n",
      "curr iteration loss: 0.19897868439413516\n",
      "curr iteration loss: 0.19797094212258343\n",
      "curr iteration loss: 0.196978382498738\n",
      "curr iteration loss: 0.19600063221519043\n",
      "curr iteration loss: 0.1950373303468676\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "curr iteration loss: 0.19408812784133017\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(100, 0.3)\n",
    "X = np.array([[1], [2], [3], [4], [5], [6]])\n",
    "y = np.array([[1], [1], [1], [0], [0], [0]])\n",
    "lr.fit(X, y)\n",
    "lr.predict(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8260ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self):\n",
    "        feat_idx_to_Y_to_X = defaultdict(lambda: defaultdict(int))\n",
    "        count_y = defaultdict(int)\n",
    "        \n",
    "    def predict(self, X, Y):\n",
    "        for y in count_y.keys():\n",
    "            for feat_idx in X.shape[1]:\n",
    "                \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        # Calculate P(X|Y) and P(Y) given X and Y\n",
    "        count_by_class = np.bincount(Y.flatten())\n",
    "        for y in np.unique(Y.flatten()):\n",
    "            y_idx = (Y == y)\n",
    "            count_y[y] = y_idx.shape[0]\n",
    "            for feat_idx in X.shape[1]:\n",
    "                relevant_X = X[y_idx, feat_idx]\n",
    "                feat_idx_to_Y_to_X[feat_idx][y] = relevant_X.shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8b349672",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1],[2], [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "5016dedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(a.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3828e34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97166c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c94082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0ef5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd0cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4585a620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c244b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f67c9aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55957957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801eba58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f23b104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150c341d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29ff8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090b357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fbe7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b96bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
